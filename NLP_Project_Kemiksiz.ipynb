{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import col,pandas_udf, PandasUDFType,count\nfrom pyspark import SparkContext,SparkConf\nfrom pyspark.sql import SparkSession\nimport nltk\n\n#Install Tagged file from databricks table\ndf = spark.table(some_file)\ndf.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>DataFrame[label: int, Text: string]\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["cleardf=df.na.drop()\ndisplay(cleardf.groupby('label').count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>label</th><th>count</th></tr></thead><tbody><tr><td>1</td><td>6367</td></tr><tr><td>0</td><td>29281</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\nalldf = cleardf.withColumn(\"label\", cleardf[\"label\"].cast(IntegerType()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["train_df, test_df = alldf.randomSplit([0.65, 0.35], seed = 2018)\n\nprint(\"Training Dataset Count: \" + str(train_df.count()))\nprint(\"Test Dataset Count: \" + str(test_df.count()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training Dataset Count: 23092\nTest Dataset Count: 12556\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["train_df.cache()\ntest_df.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>DataFrame[label: int, Text: string]\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer,StopWordsRemover,VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom mleap.sklearn.preprocessing.data import FeatureExtractor, LabelEncoder, ReshapeArrayToN1\nfrom pyspark.ml.evaluation import RegressionEvaluator,MulticlassClassificationEvaluator,BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.sql.functions import *"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["stopWordstr = StopWordsRemover.loadDefaultStopWords(\"turkish\")\n\nregexTokenizer = RegexTokenizer(inputCol=\"Text\", outputCol=\"words\", pattern=' |,|;|-|_|\\*|\\t|\\!|\\.|\\*|\\:|\\(|\\|\\\"|\\&|\\$|\\|\\#|\\}|\\]|\\[|\\)|\\{|\\/|\\'|<|>',toLowercase=True)\n\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\",stopWords =stopWordstr)\n\nhashtf = HashingTF(inputCol=\"filtered\", outputCol='tf')\n\nidf = IDF(inputCol='tf', outputCol=\"tffeatures\")\n\nva = VectorAssembler(inputCols=[\"tf\", \"tffeatures\"], outputCol=\"features\") \n\nlr = LogisticRegression()\n\npipelinelr = Pipeline(stages=[stopWordstr,regexTokenizer,remover,hashtf, idf,va,lr])\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["paramGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.006])\n             .addGrid(idf.minDocFreq,[2])\n             .addGrid(hashtf.numFeatures, [2**18])\n             .addGrid(hashtf.binary, [True])\n             .addGrid(lr.fitIntercept, [True])\n             .addGrid(lr.standardization, [True])\n             .addGrid(lr.elasticNetParam, [0.1])\n             .addGrid(lr.aggregationDepth, [2])\n             .addGrid(lr.maxIter,[1])\n             .addGrid(lr.tol,[1e-06])\n             .build()  )\n\ncvlr = CrossValidator(estimator=pipelinelr, evaluator=MulticlassClassificationEvaluator(), estimatorParamMaps=paramGrid)\n\ncvModel = cvlr.fit(train_df)\nmodellr = cvModel.bestModel"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["predictions = modellr.transform(test_df)\n \n\npredictions = predictions.select(col(\"label\").cast(\"Float\"),col(\"prediction\"))\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\n \nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy = %g\" % accuracy)\n \nevaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1 = evaluatorf1.evaluate(predictions)\nprint(\"f1 = %g\" % f1)\n \nevaluatorwp = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nwp = evaluatorwp.evaluate(predictions)\nprint(\"weightedPrecision = %g\" % wp)\n \nevaluatorwr = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nwr = evaluatorwr.evaluate(predictions)\nprint(\"weightedRecall = %g\" % wr)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%sh \nrm -rf /tmp/model_export\nmkdir /tmp/model_export"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["import sys\nsys.path.append('/opt/libs/mleap/python')\n\nimport mleap.pyspark\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer\n\n        \nmodellr.serializeToBundle(\"jar:file:/tmp/model_export/LR_model-json.zip\",predictionslr)"],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"NLP_Project_Kemiksiz","notebookId":2877063710763561},"nbformat":4,"nbformat_minor":0}
